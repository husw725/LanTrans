import streamlit as st
from openai import OpenAI
import os
import json
from pathlib import Path
from dotenv import load_dotenv
import re
from concurrent.futures import ThreadPoolExecutor
import queue
import time
load_dotenv()

# --- UI & Cost Models ---
MODEL_COST = {
    "gpt-5.1": {"input": 1.25, "output": 10.0},
    "gpt-5": {"input": 1.25, "output": 10.0},
    "gpt-5-mini": {"input": 0.25, "output": 2.0},
    "gpt-5-nano": {"input": 0.05, "output": 0.4},
}
LANG_OPTIONS = {
    "é˜¿æ‹‰ä¼¯è¯­ (Arabic)": "Arabic", "è‹±è¯­ (English)": "English", "è¥¿ç­ç‰™è¯­ (Spanish)": "Spanish",
    "è‘¡è„ç‰™è¯­ (Portuguese)": "Portuguese", "å¾·è¯­ (German)": "German", "æ³•è¯­ (French)": "French",
    "æ„å¤§åˆ©è¯­ (Italian)": "Italian", "å°å°¼è¯­ (Indonesian)": "Indonesian", "å°åœ°è¯­ (Hindi)": "Hindi",
    "æ³°è¯­ (Thai)": "Thai", "é©¬æ¥è¯­ (Malay)": "Malay", "æ—¥æœ¬è¯­ (Japanese)": "Japanese",
    "éŸ©è¯­ (Korean)": "Korean", "ä¸­æ–‡ï¼ˆç¹ä½“ï¼‰ (Traditional Chinese)": "Traditional Chinese",
    "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰ (Simplified Chinese)": "Simplified Chinese"
}

def estimate_cost(input_tokens, output_tokens, model):
    cost = (input_tokens / 1_000_000 * MODEL_COST[model]["input"]) + \
           (output_tokens / 1_000_000 * MODEL_COST[model]["output"])
    return cost

# --- Helper Function for Parallel Processing with Queue ---
def _process_single_language_with_queue(progress_queue, lang, srt_files, client, temp_dir, input_dir, output_root, translate_model, memory_model, reset):
    lang_total_cost = 0.0
    progress_queue.put({'type': 'log', 'status': 'markdown', 'message': f"--- \n### ğŸŸ¢ å¼€å§‹å¤„ç†è¯­è¨€: **{lang}**"})
    
    memory_path = temp_dir / f"drama_memory_{lang}.json"
    output_dir = Path(output_root) / lang
    output_dir.mkdir(parents=True, exist_ok=True)

    if reset and memory_path.exists():
        memory_path.unlink()
    
    try:
        memory = json.load(open(memory_path, "r", encoding="utf-8")) if memory_path.exists() else {}
    except json.JSONDecodeError:
        memory = {}
    if not memory:
        memory = {"episode_count": 0, "characters": {}, "terminology": {}, "style_notes": ""}

    for srt_file in srt_files:
        output_path = output_dir / srt_file
        if output_path.exists():
            progress_queue.put({'type': 'log', 'status': 'info', 'message': f"â¡ï¸ è·³è¿‡ {lang} - {srt_file}"})
            progress_queue.put({'type': 'progress'}) # Still count as progress
            continue

        try:
            with open(Path(input_dir) / srt_file, "r", encoding="utf-8") as f:
                srt_content = f.read()

            system_prompt = f"You are a professional subtitle translator... Current memory: {json.dumps(memory, ensure_ascii=False)}..."
            user_prompt = f"Translate the following subtitles into {lang}:\n{srt_content}"
            
            resp = client.chat.completions.create(model=translate_model, messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}])
            translated_srt = resp.choices[0].message.content.strip()
            
            cost = estimate_cost(len(system_prompt.split()) + len(user_prompt.split()), len(translated_srt.split()), translate_model)
            lang_total_cost += cost

            with open(output_path, "w", encoding="utf-8") as f:
                f.write(translated_srt)
            progress_queue.put({'type': 'log', 'status': 'success', 'message': f"âœ… å®Œæˆ {lang} - {srt_file} (è´¹ç”¨: ${cost:.4f})")

            # Attempt to Update Memory
            try:
                mem_system_prompt = "You are an assistant that updates a JSON object. ONLY output a valid, raw JSON object..."
                mem_user_prompt = f"Analyze... Previous memory: {json.dumps(memory, ensure_ascii=False)}. Translated SRT:\n{translated_srt}. Return updated JSON."
                
                upd_resp = client.chat.completions.create(model=memory_model, messages=[{"role": "system", "content": mem_system_prompt}, {"role": "user", "content": mem_user_prompt}])
                response_text = upd_resp.choices[0].message.content.strip()
                
                if response_text:
                    new_memory = json.loads(response_text)
                    memory.update(new_memory)
                    json.dump(memory, open(memory_path, "w", encoding="utf-8"), ensure_ascii=False, indent=2)
                else:
                    progress_queue.put({'type': 'log', 'status': 'warning', 'message': f"âš ï¸ {srt_file} çš„è®°å¿†æ›´æ–°è¿”å›ä¸ºç©ºã€‚"})
            except Exception as mem_e:
                progress_queue.put({'type': 'log', 'status': 'warning', 'message': f"âš ï¸ æ›´æ–° {srt_file} çš„è®°å¿†æ—¶å‡ºé”™: {mem_e}"})

        except Exception as e:
            progress_queue.put({'type': 'log', 'status': 'error', 'message': f"âŒ {lang} - {srt_file} ç¿»è¯‘å¤±è´¥: {e}"})
        
        finally:
            progress_queue.put({'type': 'progress'}) # Signal progress regardless of outcome

    progress_queue.put({'type': 'log', 'status': 'markdown', 'message': f"ğŸ’° **{lang}** æ€»è´¹ç”¨: **${lang_total_cost:.4f}**"})
    progress_queue.put({'type': 'done', 'cost': lang_total_cost})


# --- Main Application ---
def run():
    # ... (UI and client setup code remains the same) ...
    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    except Exception as e:
        st.error(f"OpenAI API å¯†é’¥åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ .env æ–‡ä»¶: {e}")
        return

    TEMP_DIR = Path("./temp")
    TEMP_DIR.mkdir(exist_ok=True)

    st.header("ğŸ“ Step 1: æ‰¹é‡å¤šè¯­è¨€ç¿»è¯‘ SRT")
    st.caption("ä½¿ç”¨ AI æ‰¹é‡ç¿»è¯‘ SRT å­—å¹•æ–‡ä»¶ï¼Œæä¾›å®æ—¶è¿›åº¦å’Œæˆæœ¬ä¼°ç®—ã€‚")

    # --- UI Layout ---
    with st.container(border=True):
        st.subheader("ğŸ“ è·¯å¾„è®¾ç½®")
        col1, col2 = st.columns(2)
        with col1:
            input_dir = st.text_input("SRT è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„ï¼š", help="å­˜æ”¾åŸå§‹ `.srt` æ–‡ä»¶çš„æ–‡ä»¶å¤¹ã€‚" )
        with col2:
            output_root = st.text_input("ç¿»è¯‘ç»“æœè¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼š", help="ç¿»è¯‘åçš„æ–‡ä»¶å°†æŒ‰è¯­è¨€ä¿å­˜åœ¨æ­¤æ–‡ä»¶å¤¹ä¸‹ã€‚" )

    with st.container(border=True):
        st.subheader("âš™ï¸ ç¿»è¯‘è®¾ç½®")
        target_displays = st.multiselect("é€‰æ‹©ç›®æ ‡è¯­è¨€ï¼ˆå¯å¤šé€‰ï¼‰", list(LANG_OPTIONS.keys()))
        target_langs = [LANG_OPTIONS[d] for d in target_displays]
        m_col1, m_col2 = st.columns(2)
        with m_col1:
            translate_model = st.selectbox("ç¿»è¯‘æ¨¡å‹", ["gpt-5.1", "gpt-5-mini", "gpt-5-nano"], index=0)
        with m_col2:
            memory_model = st.selectbox("Memory æ›´æ–°æ¨¡å‹", ["gpt-5.1", "gpt-5-mini", "gpt-5-nano"], index=0)

    with st.expander("é«˜çº§é€‰é¡¹"):
        reset = st.checkbox("æ¸…é™¤å†å²è®°å½•ï¼Œé‡æ–°ç¿»è¯‘æ‰€æœ‰æ–‡ä»¶", key="reset_all", help="å‹¾é€‰æ­¤é¡¹å°†åˆ é™¤æ‰€æœ‰è¯­è¨€çš„ç¿»è¯‘è®°å¿†ï¼Œä»å¤´å¼€å§‹ã€‚" )

    st.divider()

    if st.button("ğŸš€ å¼€å§‹æ‰¹é‡ç¿»è¯‘", type="primary", use_container_width=True):
        if not all([input_dir, output_root, target_langs]) or not os.path.exists(input_dir):
            st.warning("è¯·ç¡®ä¿æ‰€æœ‰è·¯å¾„å‡å·²æ­£ç¡®å¡«å†™ï¼Œå¹¶è‡³å°‘é€‰æ‹©ä¸€ç§ç›®æ ‡è¯­è¨€ã€‚" )
            return
        
        def natural_sort_key(s):
            return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]
        all_files = [f for f in os.listdir(input_dir) if f.lower().endswith(".srt")]
        srt_files = sorted(all_files, key=natural_sort_key)

        if not srt_files:
            st.warning("è¾“å…¥æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ° SRT æ–‡ä»¶ï¼")
            return

        # --- Concurrent Execution with Real-time Updates via Queue ---
        progress_queue = queue.Queue()
        total_files_to_process = len(srt_files) * len(target_langs)
        completed_files = 0
        
        progress_bar = st.progress(0, text="ä»»åŠ¡å‡†å¤‡å°±ç»ª...")
        log_container = st.container(height=300, border=True)
        
        total_langs_to_process = len(target_langs)
        completed_langs = 0
        total_cost_all_langs = 0.0

        with ThreadPoolExecutor(max_workers=min(total_langs_to_process, 4)) as executor:
            for lang in target_langs:
                executor.submit(_process_single_language_with_queue, progress_queue, lang, srt_files, client, TEMP_DIR, input_dir, output_root, translate_model, memory_model, reset)

            while completed_langs < total_langs_to_process:
                try:
                    msg = progress_queue.get(timeout=1.0) # Wait for a message

                    if msg['type'] == 'log':
                        status = msg.get('status', 'info')
                        if status == 'success': log_container.success(msg['message'])
                        elif status == 'info': log_container.info(msg['message'])
                        elif status == 'warning': log_container.warning(msg['message'])
                        elif status == 'error': log_container.error(msg['message'])
                        elif status == 'markdown': log_container.markdown(msg['message'])
                    
                    elif msg['type'] == 'progress':
                        completed_files += 1
                        progress_text = f"æ€»è¿›åº¦: {completed_files}/{total_files_to_process} æ–‡ä»¶å·²å¤„ç†"
                        progress_bar.progress(completed_files / total_files_to_process, text=progress_text)
                    
                    elif msg['type'] == 'done':
                        completed_langs += 1
                        total_cost_all_langs += msg.get('cost', 0)
                        
                except queue.Empty:
                    # If queue is empty for a while, it might mean tasks are done or stalled
                    pass
        
        st.balloons()
        st.success(f'''ğŸ‰ æ‰€æœ‰ç¿»è¯‘ä»»åŠ¡å®Œæˆï¼æ€»é¢„ä¼°è´¹ç”¨: ${total_cost_all_langs:.4f}'''))