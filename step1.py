import streamlit as st
from openai import OpenAI
import os
import json
from pathlib import Path
from dotenv import load_dotenv
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
load_dotenv()

# --- UI & Cost Models ---
MODEL_COST = {
    "gpt-5.1": {"input": 1.25, "output": 10.0},
    "gpt-5": {"input": 1.25, "output": 10.0},
    "gpt-5-mini": {"input": 0.25, "output": 2.0},
    "gpt-5-nano": {"input": 0.05, "output": 0.4},
}
LANG_OPTIONS = {
    "é˜¿æ‹‰ä¼¯è¯­ (Arabic)": "Arabic", "è‹±è¯­ (English)": "English", "è¥¿ç­ç‰™è¯­ (Spanish)": "Spanish",
    "è‘¡è„ç‰™è¯­ (Portuguese)": "Portuguese", "å¾·è¯­ (German)": "German", "æ³•è¯­ (French)": "French",
    "æ„å¤§åˆ©è¯­ (Italian)": "Italian", "å°å°¼è¯­ (Indonesian)": "Indonesian", "å°åœ°è¯­ (Hindi)": "Hindi",
    "æ³°è¯­ (Thai)": "Thai", "é©¬æ¥è¯­ (Malay)": "Malay", "æ—¥æœ¬è¯­ (Japanese)": "Japanese",
    "éŸ©è¯­ (Korean)": "Korean", "ä¸­æ–‡ï¼ˆç¹ä½“ï¼‰ (Traditional Chinese)": "Traditional Chinese",
    "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰ (Simplified Chinese)": "Simplified Chinese" # Added Simplified Chinese
}

def estimate_cost(input_tokens, output_tokens, model):
    cost = (input_tokens / 1_000_000 * MODEL_COST[model]["input"]) + \
           (output_tokens / 1_000_000 * MODEL_COST[model]["output"])
    return cost

# --- Helper Function for Parallel Processing ---
def _process_single_language(lang_to_process, srt_files_for_lang, client_instance, temp_dir_path, input_dir_path, output_root_path, translate_model_name, memory_model_name, reset_flag):
    lang_results = [] # To store logs for this language
    lang_total_cost = 0.0
    lang_results.append(f'''--- 
### ğŸŸ¢ å¼€å§‹å¤„ç†è¯­è¨€: **{lang_to_process}**''')
    
    memory_path = temp_dir_path / f"drama_memory_{lang_to_process}.json"
    output_dir = Path(output_root_path) / lang_to_process
    output_dir.mkdir(parents=True, exist_ok=True)

    if reset_flag and memory_path.exists():
        memory_path.unlink()
    
    try:
        memory = json.load(open(memory_path, "r", encoding="utf-8")) if memory_path.exists() else {}
    except json.JSONDecodeError:
        memory = {}
    if not memory:
        memory = {"episode_count": 0, "characters": {}, "terminology": {}, "style_notes": ""}

    for srt_file in srt_files_for_lang:
        output_path = output_dir / srt_file
        if output_path.exists():
            lang_results.append(f"â¡ï¸ è·³è¿‡ {lang_to_process} - {srt_file}")
            continue

        try:
            with open(Path(input_dir_path) / srt_file, "r", encoding="utf-8") as f:
                srt_content = f.read()

            system_prompt = f"""You are a professional subtitle translator for short dramas, specializing in localization. Your task is to translate subtitles into {lang_to_process}.
- **Translate names into a localized form that is natural and culturally appropriate for {lang_to_process} speakers.** For example, if translating 'John' to Spanish, 'Juan' might be a good option.
- Preserve the original SRT format, including timestamps.
- Maintain the original tone and style of the dialogue.
- Use the provided memory to ensure consistency for character names and terminology.
- Do not add any translator notes or any text outside of the SRT format.

Current memory: {json.dumps(memory, ensure_ascii=False)}
"""
            user_prompt = f"Translate the following subtitles:\n{srt_content}"
            
            resp = client_instance.chat.completions.create(
                model=translate_model_name,
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
            )
            translated_srt = resp.choices[0].message.content.strip()
            
            cost = estimate_cost(len(system_prompt.split()) + len(user_prompt.split()), len(translated_srt.split()), translate_model_name)
            lang_total_cost += cost

            with open(output_path, "w", encoding="utf-8") as f:
                f.write(translated_srt)
            lang_results.append(f"âœ… å®Œæˆ {lang_to_process} - {srt_file} (è´¹ç”¨: ${cost:.4f})")

            # --- Attempt to Update Memory (Safely) ---
            try:
                mem_system_prompt = "You are an assistant that updates a JSON object. ONLY output a valid, raw JSON object without explanations or markdown."
                mem_user_prompt = f"""Analyze the translated SRT and update the memory JSON.
- Identify character names and any specific terminology.
- If a name has been localized, store both the original and the localized version in the 'characters' section.
- Update the 'terminology' and 'style_notes' as needed.
- Return the complete, updated JSON object.

Previous memory: {json.dumps(memory, ensure_ascii=False)}
Translated SRT:
{translated_srt}
"""
                
                upd_resp = client_instance.chat.completions.create(
                    model=memory_model_name,
                    messages=[{"role": "system", "content": mem_system_prompt}, {"role": "user", "content": mem_user_prompt}]
                )
                response_text = upd_resp.choices[0].message.content.strip()
                
                if response_text:
                    new_memory = json.loads(response_text)
                    memory.update(new_memory)
                    json.dump(memory, open(memory_path, "w", encoding="utf-8"), ensure_ascii=False, indent=2)
                else:
                    lang_results.append(f"âš ï¸ {srt_file} çš„è®°å¿†æ›´æ–°è¿”å›ä¸ºç©ºï¼Œæœ¬æ¬¡è®°å¿†æœªæ›´æ–°ã€‚")

            except json.JSONDecodeError:
                lang_results.append(f"âš ï¸ {srt_file} çš„è®°å¿†æ›´æ–°æœªèƒ½ç”Ÿæˆæœ‰æ•ˆJSONï¼Œæœ¬æ¬¡è®°å¿†æœªæ›´æ–°ã€‚")
            except Exception as mem_e:
                lang_results.append(f"âš ï¸ æ›´æ–° {srt_file} çš„è®°å¿†æ—¶å‡ºé”™: {mem_e}ï¼Œæœ¬æ¬¡è®°å¿†æœªæ›´æ–°ã€‚")

        except Exception as e:
            lang_results.append(f"âŒ {lang_to_process} - {srt_file} ç¿»è¯‘å¤±è´¥: {e}")
            continue
    
    lang_results.append(f"ğŸ’° **{lang_to_process}** æ€»è´¹ç”¨: **${lang_total_cost:.4f}**")
    return lang_results, lang_total_cost


# --- Main Application ---
def run():
    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    except Exception as e:
        st.error(f"OpenAI API å¯†é’¥åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ .env æ–‡ä»¶: {e}")
        return

    TEMP_DIR = Path("./temp")
    TEMP_DIR.mkdir(exist_ok=True)

    st.header("ğŸ“ Step 1: æ‰¹é‡å¤šè¯­è¨€ç¿»è¯‘ SRT")
    st.caption("ä½¿ç”¨ AI æ‰¹é‡ç¿»è¯‘ SRT å­—å¹•æ–‡ä»¶ï¼Œæä¾›å®æ—¶è¿›åº¦å’Œæˆæœ¬ä¼°ç®—ã€‚")

    # --- UI Layout ---
    with st.container(border=True):
        st.subheader("ğŸ“ è·¯å¾„è®¾ç½®")
        col1, col2 = st.columns(2)
        with col1:
            input_dir = st.text_input("SRT è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„ï¼š", help="å­˜æ”¾åŸå§‹ `.srt` æ–‡ä»¶çš„æ–‡ä»¶å¤¹ã€‚" )
        with col2:
            output_root = st.text_input("ç¿»è¯‘ç»“æœè¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼š", help="ç¿»è¯‘åçš„æ–‡ä»¶å°†æŒ‰è¯­è¨€ä¿å­˜åœ¨æ­¤æ–‡ä»¶å¤¹ä¸‹ã€‚" )

    with st.container(border=True):
        st.subheader("âš™ï¸ ç¿»è¯‘è®¾ç½®")
        target_displays = st.multiselect("é€‰æ‹©ç›®æ ‡è¯­è¨€ï¼ˆå¯å¤šé€‰ï¼‰", list(LANG_OPTIONS.keys()))
        target_langs = [LANG_OPTIONS[d] for d in target_displays]
        m_col1, m_col2 = st.columns(2)
        with m_col1:
            translate_model = st.selectbox("ç¿»è¯‘æ¨¡å‹", ["gpt-5.1", "gpt-5-mini", "gpt-5-nano"], index=0)
        with m_col2:
            memory_model = st.selectbox("Memory æ›´æ–°æ¨¡å‹", ["gpt-5.1", "gpt-5-mini", "gpt-5-nano"], index=0)

    with st.expander("é«˜çº§é€‰é¡¹"):
        reset = st.checkbox("æ¸…é™¤å†å²è®°å½•ï¼Œé‡æ–°ç¿»è¯‘æ‰€æœ‰æ–‡ä»¶", key="reset_all", help="å‹¾é€‰æ­¤é¡¹å°†åˆ é™¤æ‰€æœ‰è¯­è¨€çš„ç¿»è¯‘è®°å¿†ï¼Œä»å¤´å¼€å§‹ã€‚" )

    st.divider()

    if st.button("ğŸš€ å¼€å§‹æ‰¹é‡ç¿»è¯‘", type="primary", use_container_width=True):
        if not all([input_dir, output_root, target_langs]) or not os.path.exists(input_dir):
            st.warning("è¯·ç¡®ä¿æ‰€æœ‰è·¯å¾„å‡å·²æ­£ç¡®å¡«å†™ï¼Œå¹¶è‡³å°‘é€‰æ‹©ä¸€ç§ç›®æ ‡è¯­è¨€ã€‚" )
            return
        
        def natural_sort_key(s):
            return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]
            
        all_files = [f for f in os.listdir(input_dir) if f.lower().endswith(".srt")]
        srt_files = sorted(all_files, key=natural_sort_key)

        if not srt_files:
            st.warning("è¾“å…¥æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ° SRT æ–‡ä»¶ï¼")
            return

        # --- Concurrent Execution & Display ---
        progress_bar = st.progress(0, text="ä»»åŠ¡å‡†å¤‡å°±ç»ª...")
        log_container = st.container(height=300, border=True)
        total_langs_count = len(target_langs)
        completed_langs_count = 0
        total_cost_all_langs = 0.0

        with ThreadPoolExecutor(max_workers=min(total_langs_count, 4)) as executor:
            futures = {executor.submit(_process_single_language, lang, srt_files, client, TEMP_DIR, input_dir, output_root, translate_model, memory_model, reset): lang for lang in target_langs}
            for future in as_completed(futures):
                lang = futures[future]
                try:
                    lang_results, lang_cost = future.result()
                    for msg in lang_results:
                        if "âœ…" in msg:
                            log_container.success(msg)
                        elif "â¡ï¸" in msg:
                            log_container.info(msg)
                        elif "âŒ" in msg or "âš ï¸" in msg:
                            log_container.warning(msg)
                        else:
                            log_container.markdown(msg) # For markdown formatted messages
                    total_cost_all_langs += lang_cost
                except Exception as e:
                    log_container.error(f"{lang} å¤„ç†æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                
                completed_langs_count += 1
                progress_bar.progress(completed_langs_count / total_langs_count, text=f"æ­£åœ¨ç¿»è¯‘: {completed_langs_count}/{total_langs_count} ç§è¯­è¨€å·²å®Œæˆ")

        st.balloons()
        st.success(f"ğŸ‰ æ‰€æœ‰ç¿»è¯‘ä»»åŠ¡å®Œæˆï¼æ€»é¢„ä¼°è´¹ç”¨: ${total_cost_all_langs:.4f}")
