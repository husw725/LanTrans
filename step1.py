import streamlit as st
from openai import OpenAI
import os
import json
from pathlib import Path
from dotenv import load_dotenv
import re
load_dotenv()

# --- UI & Cost Models ---
MODEL_COST = {
    "gpt-5.1": {"input": 1.25, "output": 10.0},
    "gpt-5": {"input": 1.25, "output": 10.0},
    "gpt-5-mini": {"input": 0.25, "output": 2.0},
    "gpt-5-nano": {"input": 0.05, "output": 0.4},
}
LANG_OPTIONS = {
    "é˜¿æ‹‰ä¼¯è¯­ (Arabic)": "Arabic", "è‹±è¯­ (English)": "English", "è¥¿ç­ç‰™è¯­ (Spanish)": "Spanish",
    "è‘¡è„ç‰™è¯­ (Portuguese)": "Portuguese", "å¾·è¯­ (German)": "German", "æ³•è¯­ (French)": "French",
    "æ„å¤§åˆ©è¯­ (Italian)": "Italian", "å°å°¼è¯­ (Indonesian)": "Indonesian", "å°åœ°è¯­ (Hindi)": "Hindi",
    "æ³°è¯­ (Thai)": "Thai", "é©¬æ¥è¯­ (Malay)": "Malay", "æ—¥æœ¬è¯­ (Japanese)": "Japanese",
    "éŸ©è¯­ (Korean)": "Korean", "ä¸­æ–‡ï¼ˆç¹ä½“ï¼‰ (Traditional Chinese)": "Traditional Chinese"
}

def estimate_cost(input_tokens, output_tokens, model):
    cost = (input_tokens / 1_000_000 * MODEL_COST[model]["input"]) + \
           (output_tokens / 1_000_000 * MODEL_COST[model]["output"])
    return cost

# --- Main Application ---
def run():
    # Attempt to initialize OpenAI client from environment variables
    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    except Exception as e:
        st.error(f"OpenAI API å¯†é’¥åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ .env æ–‡ä»¶: {e}")
        return

    TEMP_DIR = Path("./temp")
    TEMP_DIR.mkdir(exist_ok=True)

    st.header("ğŸ“ Step 1: æ‰¹é‡å¤šè¯­è¨€ç¿»è¯‘ SRT")
    st.caption("ä½¿ç”¨ AI æ‰¹é‡ç¿»è¯‘ SRT å­—å¹•æ–‡ä»¶ï¼Œæä¾›å®æ—¶è¿›åº¦å’Œæˆæœ¬ä¼°ç®—ã€‚")

    # --- UI Layout ---
    with st.container(border=True):
        st.subheader("ğŸ“ è·¯å¾„è®¾ç½®")
        col1, col2 = st.columns(2)
        with col1:
            input_dir = st.text_input("SRT è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„ï¼š", help="å­˜æ”¾åŸå§‹ `.srt` æ–‡ä»¶çš„æ–‡ä»¶å¤¹ã€‚")
        with col2:
            output_root = st.text_input("ç¿»è¯‘ç»“æœè¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼š", help="ç¿»è¯‘åçš„æ–‡ä»¶å°†æŒ‰è¯­è¨€ä¿å­˜åœ¨æ­¤æ–‡ä»¶å¤¹ä¸‹ã€‚")

    with st.container(border=True):
        st.subheader("âš™ï¸ ç¿»è¯‘è®¾ç½®")
        target_displays = st.multiselect("é€‰æ‹©ç›®æ ‡è¯­è¨€ï¼ˆå¯å¤šé€‰ï¼‰", list(LANG_OPTIONS.keys()))
        target_langs = [LANG_OPTIONS[d] for d in target_displays]
        m_col1, m_col2 = st.columns(2)
        with m_col1:
            translate_model = st.selectbox("ç¿»è¯‘æ¨¡å‹", ["gpt-5.1", "gpt-5-mini", "gpt-5-nano"], index=0)
        with m_col2:
            memory_model = st.selectbox("Memory æ›´æ–°æ¨¡å‹", ["gpt-5.1", "gpt-5-mini", "gpt-5-nano"], index=0)

    with st.expander("é«˜çº§é€‰é¡¹"):
        reset = st.checkbox("æ¸…é™¤å†å²è®°å½•ï¼Œé‡æ–°ç¿»è¯‘æ‰€æœ‰æ–‡ä»¶", key="reset_all", help="å‹¾é€‰æ­¤é¡¹å°†åˆ é™¤æ‰€æœ‰è¯­è¨€çš„ç¿»è¯‘è®°å¿†ï¼Œä»å¤´å¼€å§‹ã€‚")

    st.divider()

    if st.button("ğŸš€ å¼€å§‹æ‰¹é‡ç¿»è¯‘", type="primary", use_container_width=True):
        # --- Input Validation ---
        if not all([input_dir, output_root, target_langs]) or not os.path.exists(input_dir):
            st.warning("è¯·ç¡®ä¿æ‰€æœ‰è·¯å¾„å‡å·²æ­£ç¡®å¡«å†™ï¼Œå¹¶è‡³å°‘é€‰æ‹©ä¸€ç§ç›®æ ‡è¯­è¨€ã€‚")
            return
        
        # --- Natural Sort Implementation ---
        def natural_sort_key(s):
            return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]
            
        all_files = [f for f in os.listdir(input_dir) if f.lower().endswith(".srt")]
        srt_files = sorted(all_files, key=natural_sort_key)

        if not srt_files:
            st.warning("è¾“å…¥æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ° SRT æ–‡ä»¶ï¼")
            return

        # --- Sequential Processing for Real-time Feedback ---
        total_files_to_process = len(srt_files) * len(target_langs)
        files_processed = 0
        
        progress_bar = st.progress(0, text="ä»»åŠ¡å‡†å¤‡å°±ç»ª...")
        log_container = st.container(height=300, border=True)
        total_cost_all_langs = 0.0

        for lang in target_langs:
            lang_total_cost = 0.0
            log_container.markdown(f"--- \n### ğŸŸ¢ å¼€å§‹å¤„ç†è¯­è¨€: **{lang}**")
            
            memory_path = TEMP_DIR / f"drama_memory_{lang}.json"
            output_dir = Path(output_root) / lang
            output_dir.mkdir(parents=True, exist_ok=True)

            if reset and memory_path.exists():
                memory_path.unlink()
            
            try:
                memory = json.load(open(memory_path, "r", encoding="utf-8")) if memory_path.exists() else {}
            except json.JSONDecodeError:
                memory = {}
            if not memory:
                memory = {"episode_count": 0, "characters": {}, "terminology": {}, "style_notes": ""}

            for srt_file in srt_files:
                files_processed += 1
                progress_text = f"è¿›åº¦: {files_processed}/{total_files_to_process} | å½“å‰: {srt_file} ({lang})"
                progress_bar.progress(files_processed / total_files_to_process, text=progress_text)
                
                output_path = output_dir / srt_file
                if output_path.exists():
                    log_container.info(f"â¡ï¸ è·³è¿‡ {lang} - {srt_file}")
                    continue

                try:
                    with open(Path(input_dir) / srt_file, "r", encoding="utf-8") as f:
                        srt_content = f.read()

                    system_prompt = f"You are a professional subtitle translator... Current memory: {memory} ... into {lang} ..."
                    user_prompt = f"Translate the following subtitles:\n{srt_content}"
                    
                    resp = client.chat.completions.create(
                        model=translate_model,
                        messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
                    )
                    translated_srt = resp.choices[0].message.content.strip()
                    
                    cost = estimate_cost(len(system_prompt.split()) + len(user_prompt.split()), len(translated_srt.split()), translate_model)
                    lang_total_cost += cost

                    update_prompt = f"Analyze the translated SRT and update the memory... Previous: {memory} Translated:\n{translated_srt} ..."
                    upd_resp = client.chat.completions.create(
                        model=memory_model,
                        messages=[{"role": "system", "content": "You are a memory updater..."}, {"role": "user", "content": update_prompt}]
                    )
                    new_memory = json.loads(upd_resp.choices[0].message.content.strip())
                    memory.update(new_memory)
                    json.dump(memory, open(memory_path, "w", encoding="utf-8"), ensure_ascii=False, indent=2)

                    with open(output_path, "w", encoding="utf-8") as f:
                        f.write(translated_srt)
                    log_container.success(f"âœ… å®Œæˆ {lang} - {srt_file} (è´¹ç”¨: ${cost:.4f})")

                except Exception as e:
                    log_container.error(f"âŒ {lang} - {srt_file} ç¿»è¯‘å¤±è´¥: {e}")
                    continue
            
            log_container.markdown(f"ğŸ’° **{lang}** æ€»è´¹ç”¨: **${lang_total_cost:.4f}**")
            total_cost_all_langs += lang_total_cost

        st.balloons()
        st.success(f"ğŸ‰ æ‰€æœ‰ç¿»è¯‘ä»»åŠ¡å®Œæˆï¼æ€»é¢„ä¼°è´¹ç”¨: ${total_cost_all_langs:.4f}")