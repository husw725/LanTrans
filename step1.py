import streamlit as st
from openai import OpenAI
# from key import key as API_KEY
import os
import json
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
load_dotenv()

# --- UI & Cost Models ---

# æ¨¡å‹è´¹ç”¨è¡¨ (ä¿æŒä¸å˜)
MODEL_COST = {
    "gpt-5.1": {"input": 1.25, "output": 10.0},
    "gpt-5": {"input": 1.25, "output": 10.0},
    "gpt-5-mini": {"input": 0.25, "output": 2.0},
    "gpt-5-nano": {"input": 0.05, "output": 0.4},
}

LANG_OPTIONS = {
    "é˜¿æ‹‰ä¼¯è¯­ (Arabic)": "Arabic", "è‹±è¯­ (English)": "English", "è¥¿ç­ç‰™è¯­ (Spanish)": "Spanish",
    "è‘¡è„ç‰™è¯­ (Portuguese)": "Portuguese", "å¾·è¯­ (German)": "German", "æ³•è¯­ (French)": "French",
    "æ„å¤§åˆ©è¯­ (Italian)": "Italian", "å°å°¼è¯­ (Indonesian)": "Indonesian", "å°åœ°è¯­ (Hindi)": "Hindi",
    "æ³°è¯­ (Thai)": "Thai", "é©¬æ¥è¯­ (Malay)": "Malay", "æ—¥æœ¬è¯­ (Japanese)": "Japanese",
    "éŸ©è¯­ (Korean)": "Korean", "ä¸­æ–‡ï¼ˆç¹ä½“ï¼‰ (Traditional Chinese)": "Traditional Chinese"
}

def estimate_cost(input_tokens, output_tokens, model):
    cost = (input_tokens / 1_000_000 * MODEL_COST[model]["input"]) + \
           (output_tokens / 1_000_000 * MODEL_COST[model]["output"])
    return cost

# --- Main Application ---
def run():
    client = OpenAI()
    TEMP_DIR = Path("./temp")
    TEMP_DIR.mkdir(exist_ok=True)

    st.header("ğŸ“ Step 1: æ‰¹é‡å¤šè¯­è¨€ç¿»è¯‘ SRT")
    st.caption("ä½¿ç”¨ AI æ‰¹é‡ç¿»è¯‘ SRT å­—å¹•æ–‡ä»¶ï¼Œæ”¯æŒå¤šè¯­è¨€å¹¶å‘å¤„ç†å’Œæˆæœ¬ä¼°ç®—ã€‚")

    # --- UI Layout ---
    with st.container(border=True):
        st.subheader("ğŸ“ è·¯å¾„è®¾ç½®")
        col1, col2 = st.columns(2)
        with col1:
            input_dir = st.text_input("SRT è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„ï¼š", help="å­˜æ”¾åŸå§‹ `.srt` æ–‡ä»¶çš„æ–‡ä»¶å¤¹ã€‚")
        with col2:
            output_root = st.text_input("ç¿»è¯‘ç»“æœè¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼š", help="ç¿»è¯‘åçš„æ–‡ä»¶å°†æŒ‰è¯­è¨€ä¿å­˜åœ¨æ­¤æ–‡ä»¶å¤¹ä¸‹ã€‚")

    with st.container(border=True):
        st.subheader("âš™ï¸ ç¿»è¯‘è®¾ç½®")
        target_displays = st.multiselect("é€‰æ‹©ç›®æ ‡è¯­è¨€ï¼ˆå¯å¤šé€‰ï¼‰", list(LANG_OPTIONS.keys()))
        target_langs = [LANG_OPTIONS[d] for d in target_displays]

        m_col1, m_col2 = st.columns(2)
        with m_col1:
            translate_model = st.selectbox("ç¿»è¯‘æ¨¡å‹", ["gpt-5.1", "gpt-5-mini", "gpt-5-nano"], index=0)
        with m_col2:
            memory_model = st.selectbox("Memory æ›´æ–°æ¨¡å‹", ["gpt-5.1", "gpt-5-mini", "gpt-5-nano"], index=0)

    with st.expander("é«˜çº§é€‰é¡¹"):
        reset = st.checkbox("æ¸…é™¤å†å²è®°å½•ï¼Œé‡æ–°ç¿»è¯‘æ‰€æœ‰æ–‡ä»¶", key="reset_all", help="å‹¾é€‰æ­¤é¡¹å°†åˆ é™¤æ‰€æœ‰è¯­è¨€çš„ç¿»è¯‘è®°å¿†ï¼Œä»å¤´å¼€å§‹ã€‚")

    st.divider()

    if st.button("ğŸš€ å¼€å§‹æ‰¹é‡ç¿»è¯‘", type="primary", use_container_width=True):
        # --- Input Validation ---
        if not input_dir or not os.path.exists(input_dir):
            st.warning("è¯·æä¾›æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„ï¼")
            return
        if not output_root:
            st.warning("è¯·æä¾›è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼")
            return
        if not target_langs:
            st.warning("è¯·é€‰æ‹©è‡³å°‘ä¸€ç§è¯­è¨€ï¼")
            return

        srt_files = sorted([f for f in os.listdir(input_dir) if f.lower().endswith(".srt")])
        if not srt_files:
            st.warning("è¾“å…¥æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ° SRT æ–‡ä»¶ï¼")
            return

        # --- Processing Logic ---
        def process_language(lang):
            # (å¤„ç†é€»è¾‘ä¸åŸç‰ˆç›¸åŒ)
            memory_path = TEMP_DIR / f"drama_memory_{lang}.json"
            output_dir = Path(output_root) / lang
            output_dir.mkdir(parents=True, exist_ok=True)

            if reset and memory_path.exists():
                memory_path.unlink()
            
            try:
                memory = json.load(open(memory_path, "r", encoding="utf-8")) if memory_path.exists() else {}
            except json.JSONDecodeError:
                memory = {}
            
            if not memory:
                 memory = {"episode_count": 0, "characters": {}, "terminology": {}, "style_notes": ""}


            results = []
            total_cost = 0.0

            for srt_file in srt_files:
                output_path = output_dir / srt_file
                if output_path.exists():
                    results.append(f"â¡ï¸ è·³è¿‡ {lang} - {srt_file}")
                    continue

                with open(Path(input_dir) / srt_file, "r", encoding="utf-8") as f:
                    srt_content = f.read()

                system_prompt = f"You are a professional subtitle translator for short dramas. Translate subtitles into {lang} while preserving SRT format, tone, and style. Current memory: {memory}. Do not add any translator notes outside of SRT."
                user_prompt = f"Translate the following subtitles:\n{srt_content}"

                try:
                    resp = client.chat.completions.create(
                        model=translate_model,
                        messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
                    )
                    translated_srt = resp.choices[0].message.content.strip()
                    input_tokens = len(system_prompt.split()) + len(user_prompt.split())
                    output_tokens = len(translated_srt.split())
                    cost = estimate_cost(input_tokens, output_tokens, translate_model)
                    total_cost += cost

                    update_prompt = f"Analyze the following translated SRT and update the memory for characters, terminology, and style notes. Previous memory: {memory}. Translated SRT:\n{translated_srt}. Output the updated memory in JSON format."
                    upd_resp = client.chat.completions.create(
                        model=memory_model,
                        messages=[{"role": "system", "content": "You are a memory updater..."}, {"role": "user", "content": update_prompt}]
                    )
                    new_memory = json.loads(upd_resp.choices[0].message.content.strip())
                    memory.update(new_memory)
                    json.dump(memory, open(memory_path, "w", encoding="utf-8"), ensure_ascii=False, indent=2)

                    with open(output_path, "w", encoding="utf-8") as f:
                        f.write(translated_srt)
                    results.append(f"âœ… å®Œæˆ {lang} - {srt_file}, è´¹ç”¨ ${cost:.4f}")

                except Exception as e:
                    results.append(f"âŒ {lang} - {srt_file} ç¿»è¯‘å¤±è´¥: {e}")
                    continue
            
            results.append(f"ğŸ’° {lang} æ€»è´¹ç”¨: ${total_cost:.4f}")
            return results

        # --- Concurrent Execution & Display ---
        progress_bar = st.progress(0, text="ä»»åŠ¡å‡†å¤‡å°±ç»ª...")
        log_container = st.container(height=300, border=True)
        total_tasks = len(target_langs)
        completed_tasks = 0

        with ThreadPoolExecutor(max_workers=min(total_tasks, 4)) as executor:
            futures = {executor.submit(process_language, lang): lang for lang in target_langs}
            for future in as_completed(futures):
                lang = futures[future]
                try:
                    result_list = future.result()
                    for msg in result_list:
                        if "âœ…" in msg:
                            log_container.success(msg)
                        elif "â¡ï¸" in msg:
                             log_container.info(msg)
                        elif "âŒ" in msg or "âš ï¸" in msg:
                            log_container.warning(msg)
                        else:
                            log_container.write(msg)
                except Exception as e:
                    log_container.error(f"{lang} å¤„ç†æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                
                completed_tasks += 1
                progress_bar.progress(completed_tasks / total_tasks, text=f"æ­£åœ¨ç¿»è¯‘: {completed_tasks}/{total_tasks} ç§è¯­è¨€å·²å®Œæˆ")

        st.balloons()
        st.success("ğŸ‰ æ‰€æœ‰è¯­è¨€ç¿»è¯‘å®Œæˆï¼")